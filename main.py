import streamlit as st
from pypdf import PdfReader
import os
from openai import OpenAI
import psycopg2
from psycopg2.extras import RealDictCursor
import datetime

# Initialize OpenAI client with user's own API Key
api_key = os.environ.get("OPENAI_API_KEY")

if not api_key:
    # Provide a generic message about missing API keys rather than referencing Replit specifically.
    st.error("Missing OPENAI_API_KEY. Please set it as an environment variable or via your app secrets.")
    st.stop()

client = OpenAI(api_key=api_key)

# Database Setup
def get_db_connection():
    return psycopg2.connect(os.environ['DATABASE_URL'], cursor_factory=RealDictCursor)

def init_db():
    conn = get_db_connection()
    cur = conn.cursor()
    # Use IF NOT EXISTS for table creation to prevent errors on restart
    cur.execute('''
        CREATE TABLE IF NOT EXISTS study_history (
            id SERIAL PRIMARY KEY,
            subject TEXT,
            query TEXT,
            response TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS textbook_cache (
            subject TEXT PRIMARY KEY,
            content TEXT,
            filename TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    cur.close()
    conn.close()

init_db()

st.set_page_config(page_title="IB Multidisciplinary Study Assistant", layout="wide")

st.title("ğŸ“ IB Multidisciplinary Study Assistant")

# Sidebar subject and persistent upload
with st.sidebar:
    st.header("Subject & Textbook")
    subject = st.selectbox("Select Subject", ["Economics", "Business Management", "Mathematics", "Chinese", "English", "Chemistry", "Physics", "Biology", "History", "Other"])
    
    # Check if we have cached content for this subject
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("SELECT content, filename FROM textbook_cache WHERE subject = %s", (subject,))
    cached = cur.fetchone()
    cur.close()
    conn.close()

    if cached:
        st.success(f"Loaded cached textbook: {cached['filename']}")
        if st.button("Delete Cached Textbook"):
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("DELETE FROM textbook_cache WHERE subject = %s", (subject,))
            conn.commit()
            cur.close()
            conn.close()
            st.rerun()
        textbook_content = cached['content']
        uploaded_file = True # Flag for logic
    else:
        uploaded_file = st.file_uploader(
            f"Upload your {subject} textbook (PDF)",
            type=["pdf"]
        )
        if uploaded_file:
            if uploaded_file.size > 200 * 1024 * 1024:
                st.error("File too large. Please upload under 200MB.")
                st.stop()
            
            with st.spinner("Extracting and caching textbook..."):
                reader = PdfReader(uploaded_file)
                text = ""
                for page in reader.pages:
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                    except Exception:
                        continue
                
                # Save to cache
                conn = get_db_connection()
                cur = conn.cursor()
                cur.execute(                    "INSERT INTO textbook_cache (subject, content, filename) VALUES (%s, %s, %s) ON CONFLICT (subject) DO UPDATE SET content = EXCLUDED.content, filename = EXCLUDED.filename, timestamp = CURRENT_TIMESTAMP",
                    (subject, text, uploaded_file.name)
                )
                conn.commit()
                cur.close()
                conn.close()
                textbook_content = text
                st.success("PDF Cached!")
                st.rerun()

if not (cached or (locals().get('uploaded_file') and not isinstance(uploaded_file, bool))):
    st.info(f"ğŸ‘‹ Please upload your {subject} textbook in the sidebar to get started.")
    st.stop()

# History Sidebar
with st.sidebar:
    st.divider()
    st.header("ğŸ•’ Study History")
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("SELECT id, query, timestamp FROM study_history WHERE subject = %s ORDER BY timestamp DESC LIMIT 10", (subject,))
    history = cur.fetchall()
    cur.close()
    conn.close()
    
    for h in history:
        if st.button(f"{h['timestamp'].strftime('%m-%d %H:%M')}: {h['query'][:20]}...", key=f"hist_{h['id']}"):
            st.session_state.current_history_id = h['id']

# AI Analysis Tabs
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ğŸ’¡ ç®€åŒ–è§£é‡Š", "ğŸ“‘ å®Œæ•´ç†è®º", "ğŸŒ æ¡ˆä¾‹/å®éªŒ/åº”ç”¨", "ğŸ“ è€ƒè¯•å¤ä¹ ç¬”è®°", "ğŸ’¬ æ™ºèƒ½é—®ç­”", "ğŸ“œ å†å²è¯¦æƒ…"])

def save_history(subj, q, r):
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("INSERT INTO study_history (subject, query, response) VALUES (%s, %s, %s)", (subj, q, r))
    conn.commit()
    cur.close()
    conn.close()

@st.cache_data
def get_ai_response(prompt, context, subject_name, allow_external=False):
    try:
        # --- Memory System: Check for similar previous queries to save tokens ---
        conn = get_db_connection()
        cur = conn.cursor()
        # Search for queries with > 0.4 similarity (fuzzy match)
        cur.execute("""
            SELECT query, response 
            FROM study_history 
            WHERE subject = %s 
            AND similarity(query, %s) > 0.4
            ORDER BY similarity(query, %s) DESC 
            LIMIT 1
        """, (subject_name, prompt, prompt))
        prev_match = cur.fetchone()
        cur.close()
        conn.close()

        prev_context = ""
        if prev_match:
            prev_context = f"\nPREVIOUS RELATED ANSWER (for '{prev_match['query']}'):\n{prev_match['response']}\n"
            # If we have a good previous answer, we can reduce the PDF context to save tokens
            context_limit = 10000 
        else:
            context_limit = 40000

        # Improved RAG: Better keyword extraction and pattern matching
        import re
        chapter_patterns = re.findall(r'\b\d+\.\d+\b', prompt)
        keywords = [word.strip('.,?!()') for word in prompt.split() if len(word) > 3]
        
        search_terms = list(set(chapter_patterns + keywords))
        relevant_snippets = []
        
        if search_terms:
            lines = context.split('\n')
            for i, line in enumerate(lines):
                if any(re.search(re.escape(term), line, re.IGNORECASE) for term in search_terms):
                    start = max(0, i - 30)
                    end = min(len(lines), i + 50)
                    relevant_snippets.append("\n".join(lines[start:end]))
        
        if not relevant_snippets:
            context_snippet = (context[:20000] + "\n... [SNIP] ...\n" + context[-10000:])[:context_limit]
        else:
            context_snippet = ""            context_snippet = "\n--- SECTION START ---\n".join(list(set(relevant_snippets)))[:context_limit]
        
        external_instruction = ""
        if allow_external:
            external_instruction = "If the textbook context is insufficient, you MAY use external knowledge but MUST explicitly state 'ã€æ³¨ï¼šä»¥ä¸‹å†…å®¹æ¥æºäºå¤–éƒ¨èµ„æ–™ï¼Œéæ•™æåŸè¯ã€‘'."
        
        # the newest OpenAI model is "gpt-5" which was released August 7, 2025.
        # do not change this unless explicitly requested by the user
        response = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": f"You are an expert IB {subject_name} tutor. {external_instruction} Use the provided context and any previous related answers to refine your response. If a previous answer is provided, improve upon it rather than repeating it."},
                {"role": "user", "content": f"HIERARCHICAL CONTEXT FROM TEXTBOOK:\n{context_snippet}\n{prev_context}\n\nUSER REQUEST: {prompt}"}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {str(e)}"

with tab1:
    st.header(f"ğŸ’¡ {subject} ç®€åŒ–è§£é‡Š")
    topic = st.text_input("çŸ¥è¯†ç‚¹", key="topic_simple")
    if topic:
        with st.spinner("æ•´ç†ä¸­..."):
            prompt = f"ä¸­è‹±åŒè¯­è§£é‡Š '{topic}'ã€‚ä¸­æ–‡è®²é€»è¾‘ï¼Œè‹±æ–‡ç•™æœ¯è¯­ã€‚ç¦æ­¢ LaTeXã€‚"
            result = get_ai_response(prompt, textbook_content, subject)
            save_history(subject, f"ç®€åŒ–è§£é‡Š: {topic}", result)
            st.markdown(result)

with tab2:
    st.header(f"ğŸ“‘ {subject} å®Œæ•´ç†è®º")
    topic = st.text_input("ç†è®ºæ¦‚å¿µ", key="topic_theory")
    if topic:
        with st.spinner("ç”Ÿæˆä¸­..."):
            prompt = f"æä¾› '{topic}' çš„ IB è€ƒè¯•çº§ç†è®ºã€‚ä¸»ä½“å…¨è‹±æ–‡ï¼Œå…³é”®ç‚¹ä¸­æ–‡æ³¨è§£ã€‚"
            result = get_ai_response(prompt, textbook_content, subject)
            save_history(subject, f"ç†è®º: {topic}", result)
            st.markdown(result)

with tab3:
    st.header(f"ğŸŒ {subject} æ¡ˆä¾‹/å®éªŒ")
    topic = st.text_input("æ¡ˆä¾‹çŸ¥è¯†ç‚¹", key="topic_example")
    if topic:
        with st.spinner("æŸ¥æ‰¾ä¸­..."):
            prompt = f"æä¾› 2-3 ä¸ªå…³äº '{topic}' çš„è‹±æ–‡æ¡ˆä¾‹/å®éªŒï¼Œé…ä¸­æ–‡èƒŒæ™¯è¯´æ˜ã€‚"
            result = get_ai_response(prompt, textbook_content, subject)
            save_history(subject, f"æ¡ˆä¾‹: {topic}", result)
            st.markdown(result)

with tab4:
    st.header(f"ğŸ“ {subject} è¯¦ç»†å¤ä¹ ç¬”è®°")
    st.write("è¯·ç²˜è´´æ‚¨çš„è€ƒè¯•å¤§çº²ã€è€ƒé¢˜è¦æ±‚æˆ–æƒ³å¤ä¹ çš„å…·ä½“å†…å®¹ï¼ŒAI å°†ç»“åˆæ•™æä¸ºæ‚¨ç”Ÿæˆè¯¦ç»†çš„å¤ä¹ ç¬”è®°ã€‚")
    exam_content = st.text_area("è€ƒçº²/é¢˜ç›®è¦æ±‚", height=200)
    if st.button("ç”Ÿæˆè¯¦ç»†ç¬”è®°"):
        if exam_content:
            with st.spinner("æ·±åº¦æ‰«æå¹¶ç”Ÿæˆæå…¶è¯¦å°½çš„ç¬”è®°..."):
                prompt = f"æ ¹æ®æ•™æï¼Œä¸ºä»¥ä¸‹å¤§çº²ç”Ÿæˆæå…¶è¯¦å°½ã€æ— é—æ¼çš„å¤ä¹ ç¬”è®°ï¼š\n{exam_content}\n\nè¦æ±‚ï¼š\n1. å¿…é¡»æ·±å…¥åˆ°æ•™æçš„æ¯ä¸€ä¸ªå±‚çº§ï¼ˆå¤§æ ‡é¢˜ã€å°æ ‡é¢˜ã€å­è¦ç‚¹ï¼‰ï¼›\n2. åŒ…å«æ•™æä¸­æåˆ°çš„æ‰€æœ‰å®šä¹‰ã€å…¬å¼æ¨å¯¼ã€å›¾è¡¨é€»è¾‘å’Œå…·ä½“ç¤ºä¾‹ï¼›\n3. ç»“æ„ä¸¥è°¨ï¼Œä½“ç°çŸ¥è¯†çš„å±‚çº§åˆ†æ”¯ï¼ˆä¸è¦ç®€ç•¥æ¦‚æ‹¬ï¼‰ï¼›\n4. é‡‡ç”¨ä¸­è‹±åŒè¯­ï¼Œè‹±æ–‡æœ¯è¯­å¿…é¡»å‡†ç¡®ï¼›\n5. æ•™æä¸è¶³å¤„ä»¥å¤–æºèµ„æ–™è¡¥å……å¹¶æ ‡æ³¨ã€‚"
                result = get_ai_response(prompt, textbook_content, subject, allow_external=True)
                save_history(subject, f"å¤ä¹ ç¬”è®°: {exam_content[:30]}...", result)
                st.markdown(result)
        else:
            st.warning("è¯·å…ˆè¾“å…¥è€ƒè¯•å†…å®¹ã€‚")

with tab5:
    st.header(f"ğŸ’¬ {subject} æ™ºèƒ½é—®ç­”")
    user_query = st.text_input("é—®é¢˜", key="user_qa")
    if user_query:
        with st.spinner("æ€è€ƒä¸­..."):
            result = get_ai_response(user_query, textbook_content, subject, allow_external=True)
            save_history(subject, user_query, result)
            st.write("---")
            st.markdown(result)

with tab6:
    st.header("ğŸ“œ å†å²æŸ¥çœ‹")
    if 'current_history_id' in st.session_state:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("SELECT query, response, timestamp FROM study_history WHERE id = %s", (st.session_state.current_history_id,))
        record = cur.fetchone()
        cur.close()
        conn.close()
        if record:
            st.subheader(f"é—®é¢˜: {record['query']}")
            st.caption(f"æ—¶é—´: {record['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
            st.markdown(record['response'])
    else:
        st.info("åœ¨å·¦ä¾§ç‚¹å‡»å†å²è®°å½•è¿›è¡ŒæŸ¥çœ‹ã€‚")
